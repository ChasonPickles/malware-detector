from gensim.test.utils import get_tmpfile
#from gensim.test.utils import common_texts
from gensim.models import Word2Vec, KeyedVectors

# machine learning
from sklearn.ensemble import RandomForestClassifier
#from sklearn.linear_model import LogisticRegression
#from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
#from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
#from sklearn.linear_model import SGDClassifier
#from sklearn.tree import DecisionTreeClassifier
import numpy as np
import pandas as pd
import random
from collections import Counter

sentences = ["character"]
oneWord = ["character"]

df0 = pd.read_csv('result.csv')

path = get_tmpfile("word2vec.model")

#sites = [["http", "bloomberg.com", "akjdh", "/akjsdh"], ["http", "google.com", "/aksjdh", ""]]


def getVectorizedWords(listListWords: list):
    for i in listListWords:
        listListWord = [i]
        w2v = Word2Vec(listListWord, size=7, window=5, min_count=1, workers=4)
    w2vDict = w2v.wv


tmp = [df0['hostname']]
tmp.sort(key=len, reverse=True)
print(tmp)

counters = Counter(df0['hostname'])

sentences = []
for i in df0['hostname']:
    sentences.append([i])

model_bad_sites = Word2Vec(sentences, size=7, window=5, min_count=1, workers=4)
model_bad_sites.wv.save_word2vec_format('word2vec.txt', binary=False)
# model_bad_sites.wv.save_word2vec_format('word2vec.bin')
model_bad_sites_new = []

for x in set(df0['hostname']):
    tmp = model_bad_sites.wv.get_vector(x) * int(counters[x])
    model_bad_sites_new.append(tmp)

X_train_malicious = np.array(model_bad_sites_new)
Y_train_malicious = np.ones(len(X_train_malicious))


###############################################################################
# PROCESS DATA  ###############################################################
###############################################################################

####process security csv
df1 = pd.read_csv('security.csv')

security_data_safe_domain_names = df1[(df1['TIPO'] == 'Benigna')]['DOMAIN_NAME']
model_security_safe = Word2Vec(security_data_safe_domain_names, size=7, window=5, min_count=1, workers=4)


print("Number of malicious data from security.csv " + str(len(security_data_safe_domain_names)))


X_train_safe = np.array(model_security_safe.wv.vectors)
Y_train_safe = np.zeros(len(X_train_safe))

X_tuples = []
for x in X_train_malicious:
    X_tuples.append((x, 1))
for x in X_train_safe:
    X_tuples.append((x, 0))

random.shuffle(X_tuples)

tmp = [t[0] for t in X_tuples]

X_train = np.array(tmp)
Y_train = np.array([t[1] for t in X_tuples])

print("len of X_train" + str(len(X_train)))
print("len of Y_train" + str(len(Y_train)))

security_data_safe_domain_names = df1[(df1['TIPO'] == 'Benigna')]['DOMAIN_NAME'].head(150)

security_data_mal_tmp = df1[(df1['TIPO'] == 'Maligna')]['DOMAIN_NAME'].head(50)
security_data_mal = []
for x in security_data_mal_tmp:
    security_data_mal.append([x])

security_good_test_data = df1[(df1['TIPO'] == 'Benigna')]['DOMAIN_NAME'].head(250)

X_test_tmp = security_good_test_data.iloc[200:]
X_test_tmp2 = []
for x in X_test_tmp:
    X_test_tmp2.append([x])

print("X_test_tmp type" + str(type(X_test_tmp)))
X_test_tmp = np.concatenate((X_test_tmp2, security_data_mal))
tmp = Word2Vec(X_test_tmp, size=7, window=5, min_count=1, workers=4)
w2v_test = tmp.wv

X_test = []
Y_test = np.zeros(100)

for i, x in zip(range(0, 50), X_test_tmp):
    X_test.append(w2v_test[x])
    Y_test[i] = 0

for i, x in zip(range(50, 100), security_data_mal):
    X_test.append(w2v_test[x])
    Y_test[i] = 1

X_test = np.array(X_test)


for i in range(49, len(Y_test)):
    Y_test[i] = 1

test_w2v = Word2Vec(sentences, size=7, window=5, min_count=1, workers=4)
print("len(test_w2v)")
print(len(test_w2v.wv.vectors))

print("len(X_test)")
print(len(X_test))
print(X_test[0])
print("len(Y_test)")
print(len(Y_test))
print(Y_test[0].shape)

###############################################################################
# Training ####################################################################
###############################################################################

print(type(X_train))
print(type(X_train[0]))
print(type(Y_train[0]))
print(type(X_test[0]))
print(X_test)

random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, Y_train)
Y_pred = random_forest.predict(X_test)
random_forest.score(X_train, Y_train)
acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)
# print(acc_random_forest)
#
#
#knn = KNeighborsClassifier(n_neighbors=3)
#knn.fit(X_train, Y_train)
#Y_pred = knn.predict(X_test)
#acc_knn = round(knn.score(X_train, Y_train) * 100, 2)
#acc_knn
#
#perceptron = Perceptron()
#perceptron.fit(X_train, Y_train)
#Y_pred = perceptron.predict(X_test)
#acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)
#acc_perceptron

#rfc = RandomForestClassifier(n_jobs=-1, max_features= 'sqrt', n_estimators=40, oob_score=True, random_state=42)
